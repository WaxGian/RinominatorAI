```json
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéì RinominatorAI - Sistema di Training\n",
        "\n",
        "## üìã Come funziona:\n",
        "1. **Carica PDF gi√† rinominati** (esempi di training)\n",
        "2. **Il sistema impara** come hai rinominato i file\n",
        "3. **Applica** quello che ha imparato su nuovi PDF\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Istruzioni:\n",
        "1. ‚ñ∂Ô∏è Esegui le celle in ordine\n",
        "2. üìÇ Carica i tuoi PDF quando richiesto\n",
        "3. ‚è≥ Attendi il training\n",
        "4. üéâ Usa il modello per rinominare nuovi PDF!\n"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 1Ô∏è‚É£ SETUP - Installazione Dipendenze"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üîß Installazione dipendenze...\")\n",
        "print(\"‚è±Ô∏è Questo richieder√† 2-3 minuti...\\n\")\n",
        "\n",
        "!pip install -q PyMuPDF Pillow easyocr pandas python-dateutil scikit-learn numpy\n",
        "\n",
        "print(\"\\n‚úÖ Dipendenze installate!\")"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 2Ô∏è‚É£ INIZIALIZZAZIONE - Import e Setup"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import easyocr\n",
        "import re\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "print(\"‚úÖ Import completati!\")\n",
        "\n",
        "# Inizializza OCR\n",
        "print(\"\\nüîç Inizializzazione OCR (EasyOCR)...\")\n",
        "reader = easyocr.Reader(['it', 'en'], gpu=True)\n",
        "print(\"‚úÖ OCR pronto!\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3Ô∏è‚É£ FUNZIONI DI UTILIT√Ä"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_image(pdf_path):\n",
        "    \"\"\"Converti prima pagina PDF in immagine.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    page = doc[0]\n",
        "    pix = page.get_pixmap(dpi=200)\n",
        "    \n",
        "    img_path = pdf_path.replace('.pdf', '_page1.png')\n",
        "    pix.save(img_path)\n",
        "    doc.close()\n",
        "    \n",
        "    return img_path\n",
        "\n",
        "\n",
        "def extract_text_with_ocr(image_path):\n",
        "    \"\"\"Estrai testo con coordinate usando OCR.\"\"\"\n",
        "    results = reader.readtext(image_path)\n",
        "    \n",
        "    extracted = []\n",
        "    for bbox, text, confidence in results:\n",
        "        # Calcola centro del bounding box\n",
        "        x_coords = [point[0] for point in bbox]\n",
        "        y_coords = [point[1] for point in bbox]\n",
        "        \n",
        "        center_x = sum(x_coords) / len(x_coords)\n",
        "        center_y = sum(y_coords) / len(y_coords)\n",
        "        \n",
        "        # Normalizza coordinate (0-1)\n",
        "        max_x = max(x_coords)\n",
        "        max_y = max(y_coords)\n",
        "        \n",
        "        extracted.append({\n",
        "            'text': text,\n",
        "            'confidence': confidence,\n",
        "            'x': center_x / 2000,  # Normalizza\n",
        "            'y': center_y / 3000,\n",
        "            'bbox': bbox\n",
        "        })\n",
        "    \n",
        "    return extracted\n",
        "\n",
        "\n",
        "def parse_filename(filename):\n",
        "    \"\"\"Estrae denominazione, numero, data dal nome file.\"\"\"\n",
        "    name = filename.replace('.pdf', '').replace('.PDF', '')\n",
        "    \n",
        "    # Pattern 1: \"Nome NumDoc del Data\"\n",
        "    pattern1 = r'^(.+?)\\s+([A-Z0-9\\-/]+)\\s+del\\s+(.+)$'\n",
        "    match = re.match(pattern1, name, re.IGNORECASE)\n",
        "    if match:\n",
        "        return {\n",
        "            'denominazione': match.group(1).strip(),\n",
        "            'numero_documento': match.group(2).strip(),\n",
        "            'data_documento': match.group(3).strip()\n",
        "        }\n",
        "    \n",
        "    # Pattern 2: \"Nome_NumDoc_Data\"\n",
        "    pattern2 = r'^(.+?)[\\s_\\-]+([A-Z0-9\\-/]+)[\\s_\\-]+(.+)$'\n",
        "    match = re.match(pattern2, name)\n",
        "    if match:\n",
        "        parts = [p.strip() for p in [match.group(1), match.group(2), match.group(3)]]\n",
        "        date_pattern = r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}'\n",
        "        \n",
        "        if re.search(date_pattern, parts[2]):\n",
        "            return {\n",
        "                'denominazione': parts[0],\n",
        "                'numero_documento': parts[1],\n",
        "                'data_documento': parts[2]\n",
        "            }\n",
        "    \n",
        "    # Pattern 3: Cerca data e numero\n",
        "    date_match = re.search(r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}', name)\n",
        "    number_match = re.search(r'\\b[A-Z]{0,3}\\d{3,}\\b|\\b\\d{3,}\\b', name)\n",
        "    \n",
        "    if date_match and number_match:\n",
        "        data = date_match.group(0)\n",
        "        numero = number_match.group(0)\n",
        "        denom_end = name.find(numero)\n",
        "        denominazione = name[:denom_end].strip(' -_')\n",
        "        \n",
        "        return {\n",
        "            'denominazione': denominazione,\n",
        "            'numero_documento': numero,\n",
        "            'data_documento': data\n",
        "        }\n",
        "    \n",
        "    print(f\"‚ö†Ô∏è Impossibile parsificare: {filename}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def fuzzy_find_text(search_text, ocr_data, threshold=0.6):\n",
        "    \"\"\"Cerca testo nell'OCR con matching fuzzy.\"\"\"\n",
        "    search_text = search_text.lower().strip()\n",
        "    search_words = set(search_text.split())\n",
        "    \n",
        "    best_match = None\n",
        "    best_score = 0\n",
        "    \n",
        "    for item in ocr_data:\n",
        "        item_text = item['text'].lower().strip()\n",
        "        item_words = set(item_text.split())\n",
        "        \n",
        "        if search_text in item_text or item_text in search_text:\n",
        "            return item\n",
        "        \n",
        "        if not item_words or not search_words:\n",
        "            continue\n",
        "        \n",
        "        intersection = len(search_words & item_words)\n",
        "        union = len(search_words | item_words)\n",
        "        score = intersection / union\n",
        "        \n",
        "        if score > best_score and score >= threshold:\n",
        "            best_score = score\n",
        "            best_match = item\n",
        "    \n",
        "    return best_match\n",
        "\n",
        "\n",
        "def clean_filename(text):\n",
        "    \"\"\"Pulisce nome file.\"\"\"\n",
        "    text = re.sub(r'[<>:\"/\\\\\\\\|?*]', '', text)\n",
        "    return text[:150].strip()\n",
        "\n",
        "\n",
        "print(\"‚úÖ Funzioni caricate!\")"
      ],
      "metadata": {
        "id": "functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4Ô∏è‚É£ CARICA PDF DI TRAINING\n",
        "\n",
        "‚ö†Ô∏è **IMPORTANTE**: Carica almeno 10-15 PDF **gi√† rinominati correttamente**!\n",
        "\n",
        "Esempio di nomi corretti:\n",
        "- `Fornitore ABC 12345 del 15-01-2024.pdf`\n",
        "- `Azienda XYZ FT-2024-001 del 20-03-2024.pdf`\n",
        "- `Cliente DEF 99999 del 10-05-2024.pdf`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìÇ CARICA I TUOI PDF DI TRAINING\")\n",
        "print(\"(Seleziona pi√π file tenendo premuto Ctrl/Cmd)\\n\")\n",
        "\n",
        "# Crea cartella\n",
        "!mkdir -p pdf_training\n",
        "\n",
        "# Upload\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Salva file\n",
        "training_files = []\n",
        "for filename in uploaded.keys():\n",
        "    with open(f'pdf_training/{filename}', 'wb') as f:\n",
        "        f.write(uploaded[filename])\n",
        "    training_files.append(f'pdf_training/{filename}')\n",
        "\n",
        "print(f\"\\n‚úÖ Caricati {len(training_files)} PDF!\")\n",
        "\n",
        "if len(training_files) < 5:\n",
        "    print(\"\\n‚ö†Ô∏è ATTENZIONE: Hai caricato pochi PDF!\")\n",
        "    print(\"   Per un buon training servono almeno 10-15 PDF.\")\n",
        "    print(\"   Il modello potrebbe essere meno preciso.\\n\")"
      ],
      "metadata": {
        "id": "upload_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 5Ô∏è‚É£ TRAINING - Impara dai PDF\n",
        "\n",
        "Questa cella:\n",
        "1. Legge i nomi dei file\n",
        "2. Fa OCR su ogni PDF\n",
        "3. Associa nome ‚Üí contenuto\n",
        "4. Addestra modelli Machine Learning"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéì INIZIO TRAINING\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "training_data = []\n",
        "\n",
        "# Processa ogni PDF\n",
        "for i, pdf_path in enumerate(training_files, 1):\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    print(f\"üìÑ [{i}/{len(training_files)}] Analisi: {filename}\")\n",
        "    \n",
        "    try:\n",
        "        # 1. Parsifica nome\n",
        "        fields = parse_filename(filename)\n",
        "        \n",
        "        if not fields:\n",
        "            print(\"  ‚ùå Nome non parsificabile, saltato\\n\")\n",
        "            continue\n",
        "        \n",
        "        print(f\"  ‚úì Denominazione: {fields['denominazione']}\")\n",
        "        print(f\"  ‚úì Numero: {fields['numero_documento']}\")\n",
        "        print(f\"  ‚úì Data: {fields['data_documento']}\")\n",
        "        \n",
        "        # 2. OCR\n",
        "        image_path = pdf_to_image(pdf_path)\n",
        "        ocr_data = extract_text_with_ocr(image_path)\n",
        "        \n",
        "        print(f\"  ‚úì OCR: {len(ocr_data)} elementi trovati\")\n",
        "        \n",
        "        # 3. Associa\n",
        "        associations = {}\n",
        "        \n",
        "        denom = fuzzy_find_text(fields['denominazione'], ocr_data, 0.5)\n",
        "        if denom:\n",
        "            associations['denominazione'] = denom\n",
        "        \n",
        "        numero = fuzzy_find_text(fields['numero_documento'], ocr_data, 0.8)\n",
        "        if numero:\n",
        "            associations['numero_documento'] = numero\n",
        "        \n",
        "        data = fuzzy_find_text(fields['data_documento'], ocr_data, 0.7)\n",
        "        if data:\n",
        "            associations['data_documento'] = data\n",
        "        \n",
        "        training_data.append({\n",
        "            'filename': filename,\n",
        "            'fields': fields,\n",
        "            'ocr_data': ocr_data,\n",
        "            'associations': associations\n",
        "        })\n",
        "        \n",
        "        print(f\"  ‚úì Associazioni: {len(associations)}/3\\n\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Errore: {e}\\n\")\n",
        "\n",
        "print(f\"\\n‚úÖ Raccolti {len(training_data)} esempi validi!\")\n",
        "\n",
        "if len(training_data) < 3:\n",
        "    print(\"\\n‚ùå ERRORE: Troppi pochi esempi validi!\")\n",
        "    print(\"   Servono almeno 5 PDF con nomi parsificabili.\\n\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Dati pronti per il training ML!\")"
      ],
      "metadata": {
        "id": "training_collection"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 6Ô∏è‚É£ MACHINE LEARNING - Training Modelli"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nü§ñ TRAINING MODELLI MACHINE LEARNING...\\n\")\n",
        "\n",
        "# 1. Zone patterns\n",
        "print(\"  ‚îú‚îÄ Analisi zone posizionali...\")\n",
        "zone_patterns = {}\n",
        "\n",
        "for field in ['denominazione', 'numero_documento', 'data_documento']:\n",
        "    positions = []\n",
        "    for ex in training_data:\n",
        "        if field in ex['associations']:\n",
        "            item = ex['associations'][field]\n",
        "            positions.append([item['x'], item['y']])\n",
        "    \n",
        "    if positions:\n",
        "        positions = np.array(positions)\n",
        "        zone_patterns[field] = {\n",
        "            'x_range': (float(positions[:, 0].min()), float(positions[:, 0].max())),\n",
        "            'y_range': (float(positions[:, 1].min()), float(positions[:, 1].max())),\n",
        "            'x_mean': float(positions[:, 0].mean()),\n",
        "            'y_mean': float(positions[:, 1].mean())\n",
        "        }\n",
        "\n",
        "print(f\"  ‚úì Zone identificate: {len(zone_patterns)}\")\n",
        "\n",
        "# 2. Text classifiers\n",
        "print(\"\\n  ‚îú‚îÄ Training classificatori di testo...\")\n",
        "text_classifiers = {}\n",
        "vectorizers = {}\n",
        "accuracies = {}\n",
        "\n",
        "def is_fuzzy_match(text1, text2, threshold=0.6):\n",
        "    text1 = text1.lower().strip()\n",
        "    text2 = text2.lower().strip()\n",
        "    if text1 == text2 or text1 in text2 or text2 in text1:\n",
        "        return True\n",
        "    words1 = set(text1.split())\n",
        "    words2 = set(text2.split())\n",
        "    if not words1 or not words2:\n",
        "        return False\n",
        "    return (len(words1 & words2) / len(words1 | words2)) >= threshold\n",
        "\n",
        "for field in ['denominazione', 'numero_documento', 'data_documento']:\n",
        "    texts = []\n",
        "    labels = []\n",
        "    \n",
        "    for ex in training_data:\n",
        "        ground_truth = ex['fields'][field]\n",
        "        for item in ex['ocr_data']:\n",
        "            texts.append(item['text'])\n",
        "            labels.append(1 if is_fuzzy_match(item['text'], ground_truth) else 0)\n",
        "    \n",
        "    if len(set(labels)) >= 2:\n",
        "        vectorizer = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
        "        X = vectorizer.fit_transform(texts)\n",
        "        \n",
        "        clf = RandomForestClassifier(n_estimators=50, random_state=42, max_depth=10)\n",
        "        scores = cross_val_score(clf, X, labels, cv=min(3, len(training_data)))\n",
        "        acc = scores.mean()\n",
        "        \n",
        "        clf.fit(X, labels)\n",
        "        \n",
        "        text_classifiers[field] = clf\n",
        "        vectorizers[field] = vectorizer\n",
        "        accuracies[field] = acc\n",
        "        \n",
        "        print(f\"    ‚úì {field}: {acc:.1%} accuracy\")\n",
        "\n",
        "avg_accuracy = sum(accuracies.values()) / len(accuracies) if accuracies else 0\n",
        "\n",
        "print(f\"\\n  ‚úì Accuratezza media: {avg_accuracy:.1%}\")\n",
        "\n",
        "# 3. Salva modello\n",
        "print(\"\\n  ‚îú‚îÄ Salvataggio modello...\")\n",
        "\n",
        "model_data = {\n",
        "    'zone_patterns': zone_patterns,\n",
        "    'text_classifiers': text_classifiers,\n",
        "    'vectorizers': vectorizers,\n",
        "    'stats': {\n",
        "        'n_samples': len(training_data),\n",
        "        'accuracy': avg_accuracy,\n",
        "        'training_date': datetime.now().isoformat()\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('trained_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model_data, f)\n",
        "\n",
        "print(\"  ‚úì Modello salvato: trained_model.pkl\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ TRAINING COMPLETATO!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üìä STATISTICHE:\")\n",
        "print(f\"  - Documenti: {len(training_data)}\")\n",
        "print(f\"  - Accuratezza: {avg_accuracy:.1%}\")\n",
        "print(f\"  - Zone: {len(zone_patterns)}\")\n",
        "print(f\"  - Modelli: {len(text_classifiers)}\")\n",
        "print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "ml_training"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 7Ô∏è‚É£ CARICA NUOVI PDF DA RINOMINARE\n",
        "\n",
        "Ora carica i PDF con nomi generici che vuoi rinominare automaticamente!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üìÇ CARICA I PDF DA RINOMINARE\")\n",
        "print(\"(Questi sono i file che il modello rinominer√† automaticamente)\\n\")\n",
        "\n",
        "# Crea cartella\n",
        "!mkdir -p pdf_input\n",
        "\n",
        "# Upload\n",
        "uploaded_new = files.upload()\n",
        "\n",
        "# Salva\n",
        "input_files = []\n",
        "for filename in uploaded_new.keys():\n",
        "    with open(f'pdf_input/{filename}', 'wb') as f:\n",
        "        f.write(uploaded_new[filename])\n",
        "    input_files.append(f'pdf_input/{filename}')\n",
        "\n",
        "print(f\"\\n‚úÖ Caricati {len(input_files)} PDF da processare!\")"
      ],
      "metadata": {
        "id": "upload_new"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 8Ô∏è‚É£ PREDIZIONE - Rinomina Automaticamente!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ü§ñ MODALIT√Ä PREDIZIONE\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Carica modello\n",
        "with open('trained_model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "zone_patterns = model['zone_patterns']\n",
        "text_classifiers = model['text_classifiers']\n",
        "vectorizers = model['vectorizers']\n",
        "stats = model['stats']\n",
        "\n",
        "print(f\"‚úì Modello caricato (acc: {stats['accuracy']:.1%}, docs: {stats['n_samples']})\\n\")\n",
        "\n",
        "# Crea cartella output\n",
        "!mkdir -p pdf_output\n",
        "\n",
        "results = []\n",
        "\n",
        "# Processa ogni PDF\n",
        "for i, pdf_path in enumerate(input_files, 1):\n",
        "    filename = os.path.basename(pdf_path)\n",
        "    print(f\"üìÑ [{i}/{len(input_files)}] Elaborazione: {filename}\")\n",
        "    \n",
        "    try:\n",
        "        # OCR\n",
        "        print(\"  üîç Conversione + OCR...\")\n",
        "        image_path = pdf_to_image(pdf_path)\n",
        "        ocr_data = extract_text_with_ocr(image_path)\n",
        "        \n",
        "        print(\"  ü§ñ Predizione AI...\")\n",
        "        \n",
        "        # Predici ogni campo\n",
        "        predicted_fields = {}\n",
        "        confidences = {}\n",
        "        \n",
        "        for field in ['denominazione', 'numero_documento', 'data_documento']:\n",
        "            if field not in text_classifiers:\n",
        "                predicted_fields[field] = \"N/A\"\n",
        "                confidences[field] = 0.0\n",
        "                continue\n",
        "            \n",
        "            clf = text_classifiers[field]\n",
        "            vec = vectorizers[field]\n",
        "            zone = zone_patterns.get(field)\n",
        "            \n",
        "            # Filtra per zona\n",
        "            candidates = []\n",
        "            for item in ocr_data:\n",
        "                if zone:\n",
        "                    x, y = item['x'], item['y']\n",
        "                    x_min, x_max = zone['x_range']\n",
        "                    y_min, y_max = zone['y_range']\n",
        "                    margin = 0.2\n",
        "                    if x_min-margin <= x <= x_max+margin and y_min-margin <= y <= y_max+margin:\n",
        "                        candidates.append(item)\n",
        "                else:\n",
        "                    candidates.append(item)\n",
        "            \n",
        "            if not candidates:\n",
        "                candidates = ocr_data\n",
        "            \n",
        "            # Predici\n",
        "            texts = [c['text'] for c in candidates]\n",
        "            X = vec.transform(texts)\n",
        "            probs = clf.predict_proba(X)[:, 1]\n",
        "            \n",
        "            best_idx = np.argmax(probs)\n",
        "            best_prob = probs[best_idx]\n",
        "            \n",
        "            if best_prob >= 0.6:\n",
        "                predicted_fields[field] = candidates[best_idx]['text']\n",
        "                confidences[field] = best_prob\n",
        "            else:\n",
        "                predicted_fields[field] = \"N/A\"\n",
        "                confidences[field] = best_prob\n",
        "        \n",
        "        print(f\"    ‚úì Denominazione: {predicted_fields['denominazione']} ({confidences['denominazione']:.0%})\")\n",
        "        print(f\"    ‚úì Numero: {predicted_fields['numero_documento']} ({confidences['numero_documento']:.0%})\")\n",
        "        print(f\"    ‚úì Data: {predicted_fields['data_documento']} ({confidences['data_documento']:.0%})\")\n",
        "        \n",
        "        # Crea nuovo nome\n",
        "        new_name = f\"{predicted_fields['denominazione']} {predicted_fields['numero_documento']} del {predicted_fields['data_documento']}.pdf\"\n",
        "        new_name = clean_filename(new_name)\n",
        "        \n",
        "        # Copia\n",
        "        new_path = f'pdf_output/{new_name}'\n",
        "        shutil.copy2(pdf_path, new_path)\n",
        "        \n",
        "        print(f\"  ‚úÖ Salvato: {new_name}\\n\")\n",
        "        \n",
        "        results.append({\n",
        "            'original': filename,\n",
        "            'renamed': new_name,\n",
        "            'fields': predicted_fields,\n",
        "            'confidence': confidences\n",
        "        })\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Errore: {e}\\n\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ PREDIZIONE COMPLETATA!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üìä Processati: {len(results)}/{len(input_files)}\")\n",
        "print(\"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "prediction"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 9Ô∏è‚É£ SCARICA RISULTATI"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üì¶ Creazione ZIP con i risultati...\\n\")\n",
        "\n",
        "# Crea ZIP\n",
        "!zip -r pdf_rinominati.zip pdf_output/\n",
        "\n",
        "print(\"\\n‚úÖ ZIP creato!\")\n",
        "print(\"‚¨áÔ∏è Download in corso...\\n\")\n",
        "\n",
        "# Download\n",
        "files.download('pdf_rinominati.zip')\n",
        "\n",
        "print(\"\\nüéâ FATTO! Controlla i tuoi download!\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## üîü (OPZIONALE) Scarica anche il Modello Addestrato\n",
        "\n",
        "Puoi scaricare il modello per riusarlo in futuro!"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üíæ Download modello addestrato...\\n\")\n",
        "\n",
        "files.download('trained_model.pkl')\n",
        "\n",
        "print(\"‚úÖ Modello scaricato!\")\n",
        "print(\"\\nüí° Puoi ricaricare questo modello in futuro per\")\n",
        "print(\"   evitare di rifare il training!\")"
      ],
      "metadata": {
        "id": "download_model"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
```
